{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Higgs Boson Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from regression_tools import * \n",
    "from preprocessing import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(train_path,test_path):\n",
    "    train_reader=np.genfromtxt(train_path,delimiter=',',skip_header=1,converters={1:lambda s: float(0) if s==b'b' else float(1)})\n",
    "    y_train=train_reader[:,1]\n",
    "    x_train=train_reader[:,2:]\n",
    "    test_reader=np.genfromtxt(test_path,delimiter=',',skip_header=1)\n",
    "    x_test=test_reader[:,2:]\n",
    "    ids_test=test_reader[:,0]\n",
    "    return x_train,y_train,x_test,ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phi_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dc8c1c4391f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'phi_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(phi_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test di durata soluzione analitica vs SGD preparazione dati\n",
    "x_train,y_train,x_test,ids_test=load_data('train.csv','test.csv')\n",
    "x_train_cleaned,aa=cleaning_function(x_train)\n",
    "x_train_cleaned,aaa=features_augmentation(x_train_cleaned,not_augm_features=aa+1)\n",
    "phi_train=build_polinomial(x_train_cleaned,degree=12,not_poly_features=aa+aaa+1)\n",
    "lambda_=10**-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.202048063278198\n"
     ]
    }
   ],
   "source": [
    "#test di durata soluzione analitica vs SGD: calcolo soluzione analitica\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "loss,w=ridge_regression(y_train,phi_train,lambda_)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegofiori/Desktop/Machine Learning/ML_projects/ML_Higgs/regression_tools.py:127: RuntimeWarning: overflow encountered in multiply\n",
      "  dL=-tx.reshape(-1,1)*err+2*lambda_*w\n",
      "/Users/diegofiori/Desktop/Machine Learning/ML_projects/ML_Higgs/regression_tools.py:127: RuntimeWarning: invalid value encountered in multiply\n",
      "  dL=-tx.reshape(-1,1)*err+2*lambda_*w\n",
      "/Users/diegofiori/Desktop/Machine Learning/ML_projects/ML_Higgs/regression_tools.py:135: RuntimeWarning: invalid value encountered in subtract\n",
      "  w=w-gamma*g\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5a48c8ab4c20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloss_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_r\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mridge_regression_SGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphi_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/diegofiori/Desktop/Machine Learning/ML_projects/ML_Higgs/regression_tools.py\u001b[0m in \u001b[0;36mridge_regression_SGD\u001b[0;34m(y, tx, lambda_, initial_w, batch_size, max_iters, gamma)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmini_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmini_tx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_stoch_gradient_ridge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmini_tx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/diegofiori/Desktop/Machine Learning/ML_projects/ML_Higgs/regression_tools.py\u001b[0m in \u001b[0;36mbatch_iter\u001b[0;34m(y, tx, batch_size, num_batches, shuffle)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mshuffle_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mshuffled_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#test di durata soluzione analitica vs SGD: calcolo SGD\n",
    "initial_w=np.zeros(len(w))\n",
    "batch_size=1\n",
    "max_iters=100\n",
    "gamma=0.01\n",
    "start=time.time()\n",
    "loss_r, w_r =ridge_regression_SGD(y_train,phi_train,lambda_,initial_w, batch_size, max_iters, gamma)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(w_r,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train,nmc_tr=cleaning_function(x_train,-999)\n",
    "#x_test,nmc_te=cleaning_function(x_test,-999)\n",
    "# nmc= nb of not measured column, i.e. columns where \n",
    "#it is present at least one not measured element\n",
    "#phi_train=build_polinomial(x_train,4,nmc_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(phi_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LOO_cross_validation(y, phi, k_indices, k, lambda_, degree, not_poly_features):\n",
    "    \"\"\"return the loss of ridge/linear regression.\"\"\"\n",
    "    \"\"\"Probabilmente conviene implementare una per la regressione normale, in modo da capire \n",
    "    quale sia il grado massimo oltre il quale non ha senso andare e poi lavorare con lambda \n",
    "    per capire come eliminare feature\"\"\"\n",
    "    \n",
    "    \n",
    "    # Get k'th subgroup in test, others in train    \n",
    "    train_indices = np.delete(k_indices , k , 0).reshape((k_indices.shape[0]-1) * k_indices.shape[1])\n",
    "    x_test = phi[k_indices[k],:]\n",
    "    x_train = phi[train_indices,:]\n",
    "    y_test = y[k_indices[k]]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    # Form data with polynomial degree\n",
    "    tx_train = build_polinomial(x_train, degree, not_poly_features)\n",
    "    tx_test = build_polinomial(x_test, degree, not_poly_features)\n",
    "    #print(tx_test.shape)\n",
    "    #print(tx_train.shape)\n",
    "    #print(y_train.shape)\n",
    "    \n",
    "    \n",
    "    # Ridge regression / Linear regression\n",
    "    #if tx_train.shape[1]<50:\n",
    "    if lambda_!=0:\n",
    "        loss , w = ridge_regression(y_train, tx_train, lambda_)\n",
    "    else:\n",
    "        loss , w = least_squares(y_train,tx_train)\n",
    "            \n",
    "    #else: \n",
    "        # forse Ã¨ meglio implementarle all'esterno della funzione\n",
    "        #initial_w=np.ones((tx_train.shape[1]))\n",
    "        #batch_size=1\n",
    "        #max_iters=100\n",
    "        #gamma=0.01\n",
    "        #if lambda_!=0:\n",
    "        #    loss , w = ridge_regression_SGD(y_train, tx_train, lambda_,initial_w, batch_size, max_iters, gamma)\n",
    "        #else:\n",
    "        #    loss , w = least_squares_SGD(y_train,tx_train,initial_w, batch_size, max_iters, gamma)\n",
    "        \n",
    "    \n",
    "    \n",
    "    #print('REGRESSION DONE')\n",
    "    #print(y_test.shape)\n",
    "    #print(w.shape)\n",
    "    \n",
    "    # Calculate results\n",
    "    result=(y_test==(tx_test.dot(w)>0.5)).sum()/y_test.shape[0]\n",
    "    #print('RESULT CALCULATED')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#def cross_validation_degree():\n",
    "def cross_validation_demo(y_train,x_train,degrees,k_fold,lambdas,seed):\n",
    "\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "\n",
    "    # Clean data \n",
    "    x_train_cleaned,nmc_tr=cleaning_function(x_train,-999)\n",
    "    #feature augmentation\n",
    "    x_train_cleaned,noaf=features_augmentation(x_train_cleaned,not_augm_features=nmc_tr+1)\n",
    "\n",
    "    # cross validation\n",
    "    cost_te=np.zeros((lambdas.size,degrees.size))\n",
    "    for ind_lamb,lambda_ in enumerate(lambdas):\n",
    "        print(lambda_)\n",
    "        if lambda_!=0:\n",
    "            x_train_cleaned=norm_data(x_train_cleaned,not_norm_features=noaf+nmc_tr+1)\n",
    "        for ind_deg, degree_ in enumerate(degrees):\n",
    "            #print('DEGREE IS: ')\n",
    "            #print(degree_)\n",
    "            loss_te = np.zeros(k_fold)\n",
    "            for k in range (k_fold):\n",
    "                #print('K CONSIDERED IS: ')\n",
    "                #print(k)\n",
    "                result = LOO_cross_validation(y_train, x_train_cleaned, k_indices, k , lambda_, degree_, nmc_tr+1+noaf)\n",
    "                loss_te[k]= result\n",
    "\n",
    "            cost_te[ind_lamb,ind_deg]=loss_te.mean()\n",
    "    return cost_te\n",
    "\n",
    "\n",
    "    #cross_validation_degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the results  \n",
    "def plot_cross_validation(lambdas,cost_te,degrees):\n",
    "    plt.figure\n",
    "    string=[]\n",
    "    for s in range(lambdas.size):\n",
    "        plt.plot(degrees,cost_te[s])\n",
    "        string.append(str(lambdas[s]))\n",
    "    plt.xlabel('degree')\n",
    "    plt.ylabel('train accuracy')\n",
    "    plt.legend(string)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_the_maximum(matrix):\n",
    "    max_col=np.max(matrix,axis=0)\n",
    "    max_col_ind=np.max(np.argmax(max_col))\n",
    "    max_matrix=np.max(max_col)\n",
    "    max_row_ind=np.min(np.argmax(matrix[:,max_col_ind]))\n",
    "    return max_matrix,[max_row_ind,max_col_ind] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,ids_test=load_data('train.csv','test.csv')\n",
    "seed = 1\n",
    "degrees = np.arange(5,16)\n",
    "k_fold = 4\n",
    "# To use ridge regression\n",
    "lambdas = np.logspace(-8,-1,num=5)\n",
    "print(lambdas.size)\n",
    "cost_te=cross_validation_demo(y_train,x_train,degrees,k_fold,lambdas,seed)\n",
    "plot_cross_validation(lambdas,cost_te,degrees)\n",
    "_,best_param_ind=find_the_maximum(cost_te)\n",
    "print('best degree is '+str(degrees[best_param_ind[1]]))\n",
    "print('best lambda is '+str(lambdas[best_param_ind[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# continuation of the previous script\n",
    "x_train_cleaned,nmc_tr=cleaning_function(x_train,-999)\n",
    "x_train_cleaned=norm_data(x_train_cleaned,not_norm_features=nmc_tr+1)\n",
    "phi_tr=build_polinomial(x_train_cleaned,degree=degrees[best_param_ind[1]],not_poly_features=nmc_tr+1)\n",
    "loss,w=ridge_regression(y_train,phi_tr,lambdas[best_param_ind[0]])\n",
    "x_test_cleaned,nmc_te=cleaning_function(x_test,-999)\n",
    "x_test_cleaned=norm_data(x_test_cleaned,not_norm_features=nmc_te+1)\n",
    "phi_te=build_polinomial(x_test_cleaned,degree=degrees[best_param_ind[1]],not_poly_features=nmc_te+1)\n",
    "y_test=phi_te.dot(w)\n",
    "y_pred=[]\n",
    "for i in range(y_test.shape[0]):\n",
    "    if y_test[i]>0.5:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(-1)\n",
    "        #b=-1\n",
    "        \n",
    "create_csv_submission(ids_test, y_pred, 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "? np.genfromtxt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "? np.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "? np.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=np.array([1,2,3,4,5])\n",
    "b=np.concatenate([a.reshape(-1,1),a.reshape(-1,1)],axis=1)\n",
    "print(b,a[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "? np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(str('b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, tx):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.01\n",
    "    losses = []\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_regression_gradient_descent_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method_demo(y, tx):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.1\n",
    "    losses = []\n",
    "    gamma = \n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, tx, w, gamma)\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_regression_newton_method_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent_demo(y, tx):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    gamma = 0.01\n",
    "    lambda_ = 0.1\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "            \n",
    "    return loss,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_regression_penalized_gradient_descent_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_aic(y,tx,max_deg):\n",
    "    dimx=tx.shape[1]\n",
    "    loss=np.zeros(dimx) #contains loss for all models with m variables\n",
    "    best_loss=np.zeros(dimx) #contains best loss of model with m variables\n",
    "    model=[] #list of best models\n",
    "    variables=list(range(dimx)) #list of variables\n",
    "    for ind in range(dimx):\n",
    "        for m in variables:\n",
    "            loss[m],w=logistic_regression_gradient_descent_demo(y,tx[:,[model,m]])\n",
    "        b=np.argmmin(loss)\n",
    "        model.append(b)\n",
    "        variables.remove(b)\n",
    "        best_loss[ind]=loss.min()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
