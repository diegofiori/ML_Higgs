{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Higgs Boson Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from regression_tools import * \n",
    "from cross_validation_ridge import *\n",
    "from cross_validation_lasso import *\n",
    "from preprocessing import *\n",
    "from load_data import *\n",
    "from implementations import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "x_train,y_train,x_test,ids_test=load_data('train.csv','test.csv')\n",
    "# Setting parameters\n",
    "degree=20\n",
    "k_fold=3\n",
    "gamma=1e-7\n",
    "lambdas=np.logspace(-8,-1,num=5)\n",
    "seed=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "0.00173333333333\n",
      "0.00336666666667\n",
      "0.005\n"
     ]
    }
   ],
   "source": [
    "lambdas =np.logspace(-8,-1,num=5)\n",
    "gammas=np.linspace(1e-4,5e-3,num=4)\n",
    "degrees=np.arange(10,14)\n",
    "max_iters=200\n",
    "batch_size=1\n",
    "mat3D=cross_validation_lasso_demo(y_train,x_train,degrees,k_fold,lambdas,gammas,max_iters,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(gammas)):\n",
    "    plot_cross_validation(lambdas,mat3D[i],degrees,'lasso'+str(i))\n",
    "result,[best_gamma_ind,best_lambda_ind,best_degree_ind]=find_the_maximum_3D(mat3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_cross_validation(lambdas,cost_te,degrees,regression_type):\n",
    "    plt.figure()\n",
    "    string=[]\n",
    "    for s in range(lambdas.size):\n",
    "        plt.plot(degrees,cost_te[s])\n",
    "        string.append(str(lambdas[s]))\n",
    "    plt.xlabel('degree')\n",
    "    plt.ylabel('train accuracy')\n",
    "    plt.legend(string)\n",
    "    plt.savefig('cross_validation '+regression_type+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_the_maximum_3D(tensor):\n",
    "    max_mat=np.max(tensor,axis=0)\n",
    "    depth_mat=np.argmax(tensor,axis=0)\n",
    "    _,[ind_row,ind_col]=find_the_maximum(max_mat)\n",
    "    ind_depth=depth_mat[ind_row,ind_col]\n",
    "    max_tensor=np.max(tensor)\n",
    "    return max_tensor,[ind_depth,ind_row,ind_col] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1e-08 0.005 0.719990879964\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters Lasso\n",
    "print(degrees[best_degree_ind],lambdas[best_lambda_ind],gammas[best_gamma_ind],result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_cross_validation_lasso(np.log10(lambdas),mat,gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_cleaned,nmc_tr=cleaning_function(x_train,-999)\n",
    "x_train_cleaned,noaf=features_augmentation(x_train_cleaned,not_augm_features=nmc_tr+1)\n",
    "phi_train=build_polinomial(x_train_cleaned,degrees[best_degree_ind],not_poly_features=noaf+nmc_tr+1,nm=-999,already_cleaned=True)\n",
    "phi_train=norm_data(phi_train,not_norm_features=nmc_tr+1,skip_first_col=True)\n",
    "\n",
    "x_test_cleaned,nmc_te=cleaning_function(x_test,-999)\n",
    "x_test_cleaned,noaf=features_augmentation(x_test_cleaned,not_augm_features=nmc_te+1)\n",
    "phi_test=build_polinomial(x_test_cleaned,degrees[best_degree_ind],not_poly_features=noaf+nmc_te+1,nm=-999,already_cleaned=True)\n",
    "phi_test=norm_data(phi_test,not_norm_features=nmc_te+1,skip_first_col=True)\n",
    "\n",
    "loss,w=lasso_regression_SGD(y_train, phi_train, lambdas[best_lambda_idx],initial_w,max_iters,gammas[best_gamma_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test=phi_test.dot(w)\n",
    "y_pred=[]\n",
    "for i in range(y_test.shape[0]):\n",
    "    if y_test[i]>0.5:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(-1)\n",
    "        \n",
    "create_csv_submission(ids_test, y_pred, 'submission_lasso_sgd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from AIC import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,ids_test=load_data('train.csv','test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1e-08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e02ec3a0d57a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcost_te\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_validation_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplot_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_te\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ridge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mresult_ridge\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_param_ind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_the_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-055d1b4d6af1>\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[0;34m(y_train, x_train, degrees, k_fold, lambdas, seed)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_ridge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_agm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmc_tr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnoaf\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msuper_col_nb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0mloss_te\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-055d1b4d6af1>\u001b[0m in \u001b[0;36mcross_validation_ridge\u001b[0;34m(y, phi, k_indices, k, lambda_, degree, not_poly_features)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_indices\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "degrees = np.arange(5,16)\n",
    "k_fold = 4\n",
    "# To use ridge regression\n",
    "lambdas = np.logspace(-8,-1,num=5)\n",
    "print(lambdas.size)\n",
    "cost_te=cross_validation_demo(y_train,x_train,degrees,k_fold,lambdas,seed)\n",
    "plot_cross_validation(lambdas,cost_te,degrees,'ridge')\n",
    "result_ridge,best_param_ind=find_the_maximum(cost_te)\n",
    "print('best degree is '+str(degrees[best_param_ind[1]]))\n",
    "print('best lambda is '+str(lambdas[best_param_ind[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_cleaned,nmc_tr=cleaning_function(x_train,-999)\n",
    "x_train_cleaned,noac_tr=features_augmentation(x_train_cleaned,not_augm_features=nmc_tr+1)\n",
    "x_train_cleaned=norm_data(x_train_cleaned,not_norm_features=nmc_tr+1)\n",
    "phi_tr=build_polinomial(x_train_cleaned,degree=degrees[best_param_ind[1]],not_poly_features=nmc_tr+1+noac_tr)\n",
    "w,loss=ridge_regression(y_train,phi_tr,lambdas[best_param_ind[0]])\n",
    "x_test_cleaned,nmc_te=cleaning_function(x_test,-999)\n",
    "x_test_cleaned,noac_te=features_augmentation(x_test_cleaned,not_augm_features=nmc_te+1)\n",
    "x_test_cleaned=norm_data(x_test_cleaned,not_norm_features=nmc_te+1)\n",
    "phi_te=build_polinomial(x_test_cleaned,degree=degrees[best_param_ind[1]],not_poly_features=nmc_te+1+noac_te)\n",
    "y_test=phi_te.dot(w)\n",
    "y_pred=[]\n",
    "for i in range(y_test.shape[0]):\n",
    "    if y_test[i]>0.5:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(-1)\n",
    "        #b=-1\n",
    "        \n",
    "create_csv_submission(ids_test, y_pred, 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def super_features_augmentation(x,y,lambda_=0,not_super_features=0,is_train=True,augmentation=True,skip_first_column=False):\n",
    "    if skip_first_column: \n",
    "        d=1\n",
    "    else:\n",
    "        d=0\n",
    "    x_to_augm=x[:,d:x.shape[1]-not_super_features]\n",
    "    column_added=0\n",
    "    temp=(np.min(np.absolute(x_to_augm),axis=0))[:]!=0\n",
    "    is_not_zero=np.where(temp)[0]\n",
    "    log_col=np.log(np.absolute(x_to_augm[:,is_not_zero]))\n",
    "    is_zero=np.where(1- 1*temp)[0]\n",
    "    rad_col=np.sqrt(np.absolute(x_to_augm[:,is_zero]))\n",
    "    if rad_col.shape[1]>0 and log_col.shape[1]>0:\n",
    "        rad_log_col=np.concatenate((rad_col,log_col),axis=1)\n",
    "    elif rad_col.shape[1]>0:\n",
    "        rad_log_col=rad_col\n",
    "    else :\n",
    "        rad_log_col=log_col\n",
    "    if augmentation:\n",
    "        rad_log_col=features_augmentation(rad_log_col)\n",
    "    if is_train:\n",
    "        important_col=compare_aic_ridge(y_train,rad_log_col,lambda_)\n",
    "    else:\n",
    "        important_col=y\n",
    "    rad_log_col=rad_log_col[:,important_col]\n",
    "    if d>0 and not_super_features>0:\n",
    "        x=np.concatenate((x[:,:d],x_to_augm,rad_log_col,x[:,(x.shape[1]-not_super_features):]),axis=1)\n",
    "    elif d>0:\n",
    "        x=np.concatenate((x[:,:d],x_to_augm,rad_log_col),axis=1)\n",
    "    elif not_super_features>0:\n",
    "        x=np.concatenate((x_to_augm,rad_log_col,x[:,(x.shape[1]-not_super_features):]),axis=1)\n",
    "    else:\n",
    "        x=np.concatenate((x_to_augm,rad_log_col),axis=1)\n",
    "    return x, important_col\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation_ridge(y, phi, k_indices, k, lambda_, degree, not_poly_features):\n",
    "    \"\"\"\n",
    "    Return the proportion of correct classifications of ridge/linear regression in a step of k-fold cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get k'th subgroup in test, others in train    \n",
    "    train_indices = np.delete(k_indices , k , 0).reshape((k_indices.shape[0]-1) * k_indices.shape[1])\n",
    "    x_test = phi[k_indices[k],:]\n",
    "    x_train = phi[train_indices,:]\n",
    "    y_test = y[k_indices[k]]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    # Form data with polynomial degree\n",
    "    tx_train = build_polinomial(x_train, degree, not_poly_features)\n",
    "    tx_test = build_polinomial(x_test, degree, not_poly_features)\n",
    "\n",
    "    # Ridge regression / Linear regression\n",
    "    if lambda_!=0:\n",
    "        w, loss = ridge_regression(y_train, tx_train, lambda_)\n",
    "    else:\n",
    "        w, loss = least_squares(y_train,tx_train)\n",
    "   \n",
    "    \n",
    "    # Calculate proportion of correct classification for given lambda and degree\n",
    "    result=(y_test==(tx_test.dot(w)>0.5)).sum()/y_test.shape[0]\n",
    "    return result\n",
    "\n",
    "def cross_validation_demo(y_train,x_train,degrees,k_fold,lambdas,seed):\n",
    "    \"\"\"\n",
    "    Performs cross-validation with ridge regression.\n",
    "    Returns a matrix which stores the proportion of correct classifications where:\n",
    "        rows: lambda\n",
    "        columns: degree of polynomial of the features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data in k fold\n",
    "    k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "    # Clean data \n",
    "    x_train_cleaned,nmc_tr=cleaning_function(x_train,-999)\n",
    "    # Cross validation steps\n",
    "    cost_te=np.zeros((lambdas.size,degrees.size))\n",
    "    for ind_lamb,lambda_ in enumerate(lambdas):\n",
    "        print(lambda_)\n",
    "        if lambda_!=0:\n",
    "            x_train_agm,super_col=super_features_augmentation(x_train_cleaned,y_train,lambda_,not_super_features=nmc_tr+1,is_train=True,augmentation=False)\n",
    "            super_col_nb=len(super_col)\n",
    "            x_train_agm,noaf=features_augmentation(x_train_agm,not_augm_features=nmc_tr+1)\n",
    "            x_train_agm=norm_data(x_train_agm,not_norm_features=nmc_tr+1)\n",
    "        for ind_deg, degree_ in enumerate(degrees):\n",
    "            loss_te = np.zeros(k_fold)\n",
    "            for k in range (k_fold):\n",
    "                result = cross_validation_ridge(y_train, x_train_agm, k_indices, k , lambda_, degree_, nmc_tr+1+noaf+super_col_nb)\n",
    "                loss_te[k]= result\n",
    "\n",
    "            cost_te[ind_lamb,ind_deg]=loss_te.mean()\n",
    "    return cost_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test logistic\n",
    "degree=13\n",
    "x_train_cleaned,nmc_tr=cleaning_function(x_train,-999)\n",
    "#x_train_cleaned,noaf=features_augmentation(x_train_cleaned,not_augm_features=nmc_tr+1)\n",
    "#print(x_train_cleaned.shape)\n",
    "#phi_train=build_polinomial(x_train_cleaned,degree,not_poly_features=noaf+nmc_tr+1,nm=-999,already_cleaned=True)\n",
    "#print(phi_train.shape)\n",
    "#phi_train=norm_data(phi_train,not_norm_features=nmc_tr+1,skip_first_col=True)\n",
    "#print(phi_train.shape)\n",
    "phi_train=x_train_cleaned\n",
    "initial_w=np.zeros(phi_train.shape[1])\n",
    "batch_size=1\n",
    "max_iters=100\n",
    "gamma=1e-4\n",
    "w,loss=logistic_regression(y_train, phi_train, initial_w, max_iters, gamma)\n",
    "result=((phi_train.dot(w)[:]>0.5)[:]==y_train[:]).sum()/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print((w[:]==0).sum(),result)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81% DO NOT TOUCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,ids_test=load_data('train.csv','test.csv')\n",
    "seed = 1\n",
    "degrees = np.arange(5,16)\n",
    "k_fold = 4\n",
    "# To use ridge regression\n",
    "lambdas = np.logspace(-8,-1,num=5)\n",
    "print(lambdas.size)\n",
    "cost_te=cross_validation_demo(y_train,x_train,degrees,k_fold,lambdas,seed)\n",
    "plot_cross_validation(lambdas,cost_te,degrees,'ridge')\n",
    "_,best_param_ind=find_the_maximum(cost_te)\n",
    "print('best degree is '+str(degrees[best_param_ind[1]]))\n",
    "print('best lambda is '+str(lambdas[best_param_ind[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.min(w),np.max(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# continuation of the previous script\n",
    "x_train_cleaned,nmc_tr=cleaning_function(x_train,-999)\n",
    "x_train_cleaned,noac_tr=features_augmentation(x_train_cleaned,not_augm_features=nmc_tr+1)\n",
    "x_train_cleaned=norm_data(x_train_cleaned,not_norm_features=nmc_tr+1)\n",
    "phi_tr=build_polinomial(x_train_cleaned,degree=degrees[best_param_ind[1]],not_poly_features=nmc_tr+1+noac_tr)\n",
    "loss,w=ridge_regression(y_train,phi_tr,lambdas[best_param_ind[0]])\n",
    "x_test_cleaned,nmc_te=cleaning_function(x_test,-999)\n",
    "x_test_cleaned,noac_te=features_augmentation(x_test_cleaned,not_augm_features=nmc_te+1)\n",
    "x_test_cleaned=norm_data(x_test_cleaned,not_norm_features=nmc_te+1)\n",
    "phi_te=build_polinomial(x_test_cleaned,degree=degrees[best_param_ind[1]],not_poly_features=nmc_te+1+noac_te)\n",
    "y_test=phi_te.dot(w)\n",
    "y_pred=[]\n",
    "for i in range(y_test.shape[0]):\n",
    "    if y_test[i]>0.5:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(-1)\n",
    "        #b=-1\n",
    "        \n",
    "create_csv_submission(ids_test, y_pred, 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logistic_regression_penalized_gradient_descent_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,ids_test=load_data('train.csv','test.csv')\n",
    "x_train_cleaned,aa=cleaning_function(x_train)\n",
    "tx=build_polinomial(x_train_cleaned,degree=1,not_poly_features=0)\n",
    "tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_aic(y_train,tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=list([1,2,3])\n",
    "a.append(4)\n",
    "print(a)\n",
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[]\n",
    "a.append(0)\n",
    "\n",
    "temp=a.copy()\n",
    "temp.append(2)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_aic(y,tx):\n",
    "    dimx=tx.shape[1]\n",
    "    loss=np.zeros(dimx) #contains loss for all models with m variables\n",
    "    best_loss=np.zeros(dimx) #contains best loss of model with m variables\n",
    "    models=[] #list of best models\n",
    "    variables=list(range(dimx)) #list of variables\n",
    "    for ind in range(dimx):\n",
    "        for m in variables:\n",
    "            temp=models.copy()\n",
    "            #print(m)\n",
    "            #print(ind)\n",
    "            temp.append(m)\n",
    "            #print(temp)\n",
    "            #print(tx[:,temp].shape)\n",
    "            [loss[m],w]=logistic_regression_gradient_descent_demo(y,tx[:,temp])\n",
    "        b=np.argmmin(loss)\n",
    "        models.append(b)\n",
    "        variables.remove(b)\n",
    "        best_loss[ind]=loss.min()\n",
    "        \n",
    "    idx_loss=np.argmin(best_loss)\n",
    "    model=models[:idx_loss]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    \"\"\"\n",
    "    Computes the sign() function.\n",
    "    \"\"\"\n",
    "    true_vec1=x[:]>0\n",
    "    true_vec2=x[:]<0\n",
    "    x=1*true_vec1-1*true_vec2\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=np.zeros((3,2,2))\n",
    "a[2,1,1]=1\n",
    "print(np.argmax(a,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "? np.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "a=np.arange(4)\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
