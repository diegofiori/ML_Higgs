{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Higgs Boson Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from regression_tools import * \n",
    "from cross_validation_ridge import *\n",
    "from preprocessing import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(train_path,test_path):\n",
    "    train_reader=np.genfromtxt(train_path,delimiter=',',skip_header=1,converters={1:lambda s: float(0) if s==b'b' else float(1)})\n",
    "    y_train=train_reader[:,1]\n",
    "    x_train=train_reader[:,2:]\n",
    "    test_reader=np.genfromtxt(test_path,delimiter=',',skip_header=1)\n",
    "    x_test=test_reader[:,2:]\n",
    "    ids_test=test_reader[:,0]\n",
    "    return x_train,y_train,x_test,ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(phi_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test di durata soluzione analitica vs SGD preparazione dati\n",
    "x_train,y_train,x_test,ids_test=load_data('train.csv','test.csv')\n",
    "x_train_cleaned,aa=cleaning_function(x_train)\n",
    "x_train_cleaned,aaa=features_augmentation(x_train_cleaned,not_augm_features=aa+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phi_train=build_polinomial(x_train_cleaned,degree=12,not_poly_features=aa+aaa+1)\n",
    "phi_train=norm_data(phi_train,skip_first_col=True,not_norm_features=aa+1)\n",
    "lambda_=10**-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_data(x,skip_first_col=False,not_norm_features=0):\n",
    "    if skip_first_col:\n",
    "        beg=1\n",
    "    else:\n",
    "        beg=0\n",
    "    x_to_norm=x[:,beg:x.shape[1]-not_norm_features]\n",
    "    means=np.mean(x_to_norm,axis=0)\n",
    "    sds=np.std(x_to_norm,axis=0)\n",
    "    x_to_norm=(x_to_norm-means.reshape(1,-1))/sds.reshape(1,-1)\n",
    "    x[:,beg:x.shape[1]-not_norm_features]=x_to_norm\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "loss,w=logistic_regression_gradient_descent_demo(y_train, phi_train,max_iter = 10000,\n",
    "    threshold = 1e-8,gamma=10**-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test di durata soluzione analitica vs SGD: calcolo soluzione analitica\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "loss,w=ridge_regression(y_train,phi_train,lambda_)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test di durata soluzione analitica vs SGD: calcolo SGD\n",
    "initial_w=np.zeros(len(w))\n",
    "batch_size=1\n",
    "max_iters=100\n",
    "gamma=0.01\n",
    "start=time.time()\n",
    "loss_r, w_r =ridge_regression_SGD(y_train,phi_train,lambda_,initial_w, batch_size, max_iters, gamma)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(w_r,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train,nmc_tr=cleaning_function(x_train,-999)\n",
    "#x_test,nmc_te=cleaning_function(x_test,-999)\n",
    "# nmc= nb of not measured column, i.e. columns where \n",
    "#it is present at least one not measured element\n",
    "#phi_train=build_polinomial(x_train,4,nmc_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_cross_validation(y, phi, k_indices, k, lambda_, degree, not_poly_features):\n",
    "    \"\"\"return the loss of ridge/linear regression.\"\"\"\n",
    "    \"\"\"Probabilmente conviene implementare una per la regressione normale, in modo da capire \n",
    "    quale sia il grado massimo oltre il quale non ha senso andare e poi lavorare con lambda \n",
    "    per capire come eliminare feature\"\"\"\n",
    "    \n",
    "    \n",
    "        \n",
    "    train_indices = np.delete(k_indices , k , 0).reshape((k_indices.shape[0]-1) * k_indices.shape[1])\n",
    "    x_test = phi[k_indices[k],:]\n",
    "    x_train = phi[train_indices,:]\n",
    "    y_test = y[k_indices[k]]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    \n",
    "    tx_train = build_polinomial(x_train, degree, not_poly_features)\n",
    "    tx_test = build_polinomial(x_test, degree, not_poly_features)\n",
    "    \n",
    "    if lambda_!=0:\n",
    "        loss , w = ridge_regression(y_train, tx_train, lambda_)\n",
    "    else:\n",
    "        loss , w = least_squares(y_train,tx_train)\n",
    "            \n",
    "    result=(y_test==(tx_test.dot(w)>0.5)).sum()/y_test.shape[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#def cross_validation_degree():\n",
    "def cross_validation_logistic_demo(y_train,x_train,degrees,k_fold,lambdas,seed):\n",
    "\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "\n",
    "    # Clean data \n",
    "    x_train_cleaned,nmc_tr=cleaning_function(x_train,-999)\n",
    "    #feature augmentation\n",
    "    x_train_cleaned,noaf=features_augmentation(x_train_cleaned,not_augm_features=nmc_tr+1)\n",
    "\n",
    "    # cross validation\n",
    "    cost_te=np.zeros((lambdas.size,degrees.size))\n",
    "    for ind_lamb,lambda_ in enumerate(lambdas):\n",
    "        print(lambda_)\n",
    "        if lambda_!=0:\n",
    "            x_train_cleaned=norm_data(x_train_cleaned,not_norm_features=noaf+nmc_tr+1)\n",
    "        for ind_deg, degree_ in enumerate(degrees):\n",
    "            #print('DEGREE IS: ')\n",
    "            #print(degree_)\n",
    "            loss_te = np.zeros(k_fold)\n",
    "            for k in range (k_fold):\n",
    "                #print('K CONSIDERED IS: ')\n",
    "                #print(k)\n",
    "                result = LOO_cross_validation(y_train, x_train_cleaned, k_indices, k , lambda_, degree_, nmc_tr+1+noaf)\n",
    "                loss_te[k]= result\n",
    "\n",
    "            cost_te[ind_lamb,ind_deg]=loss_te.mean()\n",
    "    return cost_te\n",
    "\n",
    "\n",
    "    #cross_validation_degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the results  \n",
    "def plot_cross_validation(lambdas,cost_te,degrees):\n",
    "    plt.figure\n",
    "    string=[]\n",
    "    for s in range(lambdas.size):\n",
    "        plt.plot(degrees,cost_te[s])\n",
    "        string.append(str(lambdas[s]))\n",
    "    plt.xlabel('degree')\n",
    "    plt.ylabel('train accuracy')\n",
    "    plt.legend(string)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_the_maximum(matrix):\n",
    "    max_col=np.max(matrix,axis=0)\n",
    "    max_col_ind=np.max(np.argmax(max_col))\n",
    "    max_matrix=np.max(max_col)\n",
    "    max_row_ind=np.min(np.argmax(matrix[:,max_col_ind]))\n",
    "    return max_matrix,[max_row_ind,max_col_ind] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81% DO NOT TOUCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "x_train,y_train,x_test,ids_test=load_data('train.csv','test.csv')\n",
    "seed = 1\n",
    "degrees = np.arange(5,16)\n",
    "k_fold = 4\n",
    "# To use ridge regression\n",
    "lambdas = np.logspace(-8,-1,num=5)\n",
    "print(lambdas.size)\n",
    "cost_te=cross_validation_demo(y_train,x_train,degrees,k_fold,lambdas,seed)\n",
    "plot_cross_validation(lambdas,cost_te,degrees)\n",
    "_,best_param_ind=find_the_maximum(cost_te)\n",
    "print('best degree is '+str(degrees[best_param_ind[1]]))\n",
    "print('best lambda is '+str(lambdas[best_param_ind[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# continuation of the previous script\n",
    "x_train_cleaned,nmc_tr=cleaning_function(x_train,-999)\n",
    "x_train_cleaned=norm_data(x_train_cleaned,not_norm_features=nmc_tr+1)\n",
    "phi_tr=build_polinomial(x_train_cleaned,degree=degrees[best_param_ind[1]],not_poly_features=nmc_tr+1)\n",
    "loss,w=ridge_regression(y_train,phi_tr,lambdas[best_param_ind[0]])\n",
    "x_test_cleaned,nmc_te=cleaning_function(x_test,-999)\n",
    "x_test_cleaned=norm_data(x_test_cleaned,not_norm_features=nmc_te+1)\n",
    "phi_te=build_polinomial(x_test_cleaned,degree=degrees[best_param_ind[1]],not_poly_features=nmc_te+1)\n",
    "y_test=phi_te.dot(w)\n",
    "y_pred=[]\n",
    "for i in range(y_test.shape[0]):\n",
    "    if y_test[i]>0.5:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(-1)\n",
    "        #b=-1\n",
    "        \n",
    "create_csv_submission(ids_test, y_pred, 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "? np.genfromtxt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "? np.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "? np.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=np.array([1,2,3,4,5])\n",
    "b=np.concatenate([a.reshape(-1,1),a.reshape(-1,1)],axis=1)\n",
    "print(b,a[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "? np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(str('b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent_demo(y, tx,gamma,max_iter,threshold):\n",
    "    # init parameters\n",
    "    losses = []\n",
    "    w = np.zeros((tx.shape[1],1))\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y.reshape(-1,1), tx, w, gamma)\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logistic_regression_gradient_descent_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method_demo(y, tx):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.1\n",
    "    losses = []\n",
    "    gamma = 1\n",
    "    w = np.zeros((tx.shape[1],1))\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y.reshape(-1,1), tx, w, gamma)\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logistic_regression_newton_method_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent_demo(y, tx):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    gamma = 0.01\n",
    "    lambda_ = 0.1\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "            \n",
    "    return loss,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logistic_regression_penalized_gradient_descent_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,ids_test=load_data('train.csv','test.csv')\n",
    "x_train_cleaned,aa=cleaning_function(x_train)\n",
    "tx=build_polinomial(x_train_cleaned,degree=1,not_poly_features=0)\n",
    "tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_aic(y_train,tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=list([1,2,3])\n",
    "a.append(4)\n",
    "print(a)\n",
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[]\n",
    "a.append(0)\n",
    "\n",
    "temp=a.copy()\n",
    "temp.append(2)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_aic(y,tx):\n",
    "    dimx=tx.shape[1]\n",
    "    loss=np.zeros(dimx) #contains loss for all models with m variables\n",
    "    best_loss=np.zeros(dimx) #contains best loss of model with m variables\n",
    "    models=[] #list of best models\n",
    "    variables=list(range(dimx)) #list of variables\n",
    "    for ind in range(dimx):\n",
    "        for m in variables:\n",
    "            temp=models.copy()\n",
    "            #print(m)\n",
    "            #print(ind)\n",
    "            temp.append(m)\n",
    "            #print(temp)\n",
    "            #print(tx[:,temp].shape)\n",
    "            [loss[m],w]=logistic_regression_gradient_descent_demo(y,tx[:,temp])\n",
    "        b=np.argmmin(loss)\n",
    "        models.append(b)\n",
    "        variables.remove(b)\n",
    "        best_loss[ind]=loss.min()\n",
    "        \n",
    "    idx_loss=np.argmin(best_loss)\n",
    "    model=models[:idx_loss]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
